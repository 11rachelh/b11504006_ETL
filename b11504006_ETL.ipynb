{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fb4209-0af8-496c-99b6-2a0e0578dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC4863427.html (180,248 bytes)\n",
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC7223160.html (357,542 bytes)\n",
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC7223160.html (357,542 bytes)\n",
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC7264388.html (205,454 bytes)\n",
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC7264388.html (205,454 bytes)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# List of PMC article URLs\n",
    "urls = [\n",
    "    \"https://pmc.ncbi.nlm.nih.gov/articles/PMC4863427\",\n",
    "    \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7223160\",\n",
    "    \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7264388\",\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/125.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://pmc.ncbi.nlm.nih.gov/\",\n",
    "}\n",
    "\n",
    "for url in urls:\n",
    "    pmcid = url.split(\"/\")[-1]\n",
    "    outfile = Path(f\"{pmcid}.html\")\n",
    "    with requests.Session() as s:\n",
    "        s.headers.update(headers)\n",
    "        resp = s.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        if not resp.encoding:\n",
    "            resp.encoding = \"utf-8\"\n",
    "        outfile.write_text(resp.text, encoding=resp.encoding)\n",
    "    print(f\"Saved {outfile.resolve()} ({outfile.stat().st_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662a8432-f747-488e-999f-675b13decccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to b11504006_info_data.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "html_files = [\n",
    "    'PMC4863427.html',\n",
    "    'PMC7223160.html',\n",
    "    'PMC7264388.html',\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# --------- 工具函式 ---------\n",
    "def normalize_number(val: str) -> str:\n",
    "    \"\"\"將各種奇怪的負號、空白、註腳去掉\"\"\"\n",
    "    val = val.strip()\n",
    "    val = val.replace('−', '-').replace('–', '-')\n",
    "    val = re.sub(r'\\u00a0', ' ', val)  # 移除 &nbsp;\n",
    "    val = re.sub(r'[\\*\\†\\‡\\^a-zA-Z]+$', '', val)  # 移除尾端註腳\n",
    "    return val\n",
    "\n",
    "def is_number(val: str) -> bool:\n",
    "    \"\"\"判斷是否為數字\"\"\"\n",
    "    try:\n",
    "        float(normalize_number(val))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def extract_keywords(soup, html_text):\n",
    "    meta_keywords = soup.find('meta', attrs={'name': 'citation_keywords'})\n",
    "    if meta_keywords and meta_keywords.get('content'):\n",
    "        return meta_keywords.get('content')\n",
    "\n",
    "    for tag in soup.find_all(['b', 'strong', 'h3', 'h4', 'p', 'span']):\n",
    "        if tag.get_text(strip=True).lower().startswith('keywords'):\n",
    "            next_text = ''\n",
    "            text = tag.get_text(separator=' ', strip=True)\n",
    "            if ':' in text:\n",
    "                next_text = text.split(':', 1)[1].strip()\n",
    "            if not next_text:\n",
    "                sibling = tag.find_next_sibling(text=True)\n",
    "                if sibling:\n",
    "                    next_text = sibling.strip()\n",
    "            if not next_text and tag.parent:\n",
    "                sibling = tag.parent.find_next_sibling(text=True)\n",
    "                if sibling:\n",
    "                    next_text = sibling.strip()\n",
    "            if next_text:\n",
    "                return next_text\n",
    "\n",
    "    kw_match = re.search(r'Keywords?[:\\s]+([\\w\\s;,-]+)', html_text, re.IGNORECASE)\n",
    "    if kw_match:\n",
    "        return kw_match.group(1).strip()\n",
    "\n",
    "    return ''\n",
    "\n",
    "def find_all_xyz_indices(header_rows):\n",
    "    \"\"\" 從多行表頭中找出所有 X,Y,Z 的索引位置（可能有 Left/Right 兩組） \"\"\"\n",
    "    last_row = header_rows[-1]  # 最底層表頭\n",
    "    flat_headers = [cell.strip().lower() for cell in last_row]\n",
    "\n",
    "    xyz_groups = []\n",
    "    temp = {}\n",
    "    for i, h in enumerate(flat_headers):\n",
    "        if h == 'x':\n",
    "            temp['x'] = i\n",
    "        elif h == 'y':\n",
    "            temp['y'] = i\n",
    "        elif h == 'z':\n",
    "            temp['z'] = i\n",
    "\n",
    "        if len(temp) == 3:  # 收集到一組\n",
    "            xyz_groups.append((temp['x'], temp['y'], temp['z']))\n",
    "            temp = {}\n",
    "\n",
    "    return xyz_groups if xyz_groups else None\n",
    "\n",
    "# --------- 主程式 ---------\n",
    "for html_file in html_files:\n",
    "    html_path = Path(html_file)\n",
    "    if not html_path.exists():\n",
    "        print(f\"File not found: {html_file}\")\n",
    "        continue\n",
    "\n",
    "    html_text = html_path.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "    # PMCID\n",
    "    pmcid = re.search(r'PMC\\d+', html_file).group() if re.search(r'PMC\\d+', html_file) else ''\n",
    "\n",
    "    # PMID\n",
    "    pmid = ''\n",
    "    meta_pmid = soup.find('meta', attrs={'name': 'citation_pmid'})\n",
    "    if meta_pmid:\n",
    "        pmid = meta_pmid.get('content')\n",
    "    else:\n",
    "        pmid_match = re.search(r'PMID[:\\s]+(\\d+)', html_text)\n",
    "        if pmid_match:\n",
    "            pmid = pmid_match.group(1)\n",
    "\n",
    "    # Keywords\n",
    "    keywords = extract_keywords(soup, html_text)\n",
    "\n",
    "    # 找表格\n",
    "    tables = soup.find_all('table')\n",
    "    for table_idx, table in enumerate(tables, 1):\n",
    "        rows = table.find_all('tr')\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        # header rows → 把 thead 和前兩行都加進來\n",
    "        header_rows = []\n",
    "        thead = table.find('thead')\n",
    "        if thead:\n",
    "            for row in thead.find_all('tr'):\n",
    "                header_rows.append([cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])])\n",
    "        if not header_rows:\n",
    "            for row in rows[:2]:\n",
    "                cells = row.find_all(['th', 'td'])\n",
    "                if all(cell.name == 'th' for cell in cells):\n",
    "                    header_rows.append([cell.get_text(strip=True) for cell in cells])\n",
    "        if not header_rows:\n",
    "            header_rows = [[cell.get_text(strip=True) for cell in rows[0].find_all(['th', 'td'])]]\n",
    "\n",
    "        xyz_groups = find_all_xyz_indices(header_rows)\n",
    "\n",
    "        if xyz_groups:\n",
    "            for row in rows[len(header_rows):]:\n",
    "                cols = [td.get_text(strip=True) for td in row.find_all(['td', 'th'])]\n",
    "                for (x_idx, y_idx, z_idx) in xyz_groups:\n",
    "                    if len(cols) > max(x_idx, y_idx, z_idx):\n",
    "                        x_val, y_val, z_val = cols[x_idx], cols[y_idx], cols[z_idx]\n",
    "\n",
    "                        # ✅ 每組獨立成一筆 row\n",
    "                        if any(is_number(v) for v in [x_val, y_val, z_val]):\n",
    "                            results.append({\n",
    "                                'PMID': pmid,\n",
    "                                'PMCID': pmcid.replace('PMC',''),\n",
    "                                'Keywords': keywords,\n",
    "                                'Table': table_idx,\n",
    "                                'X': normalize_number(x_val) if is_number(x_val) else '',\n",
    "                                'Y': normalize_number(y_val) if is_number(y_val) else '',\n",
    "                                'Z': normalize_number(z_val) if is_number(z_val) else '',\n",
    "                            })\n",
    "\n",
    "# 存檔\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('b11504006_info_data.csv', index=False, encoding='utf-8-sig')\n",
    "print('Saved to b11504006_info_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cad97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
