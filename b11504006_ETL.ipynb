{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec7688e-2afe-471f-ba09-8ea51e5fbd71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fb4209-0af8-496c-99b6-2a0e0578dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC4863427.html (180,248 bytes)\n",
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC7223160.html (357,542 bytes)\n",
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC7223160.html (357,542 bytes)\n",
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC7264388.html (205,454 bytes)\n",
      "Saved C:\\Users\\USER\\github-classroom\\ntu-info\\neurosynth-etl-11rachelh\\PMC7264388.html (205,454 bytes)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# List of PMC article URLs\n",
    "urls = [\n",
    "    \"https://pmc.ncbi.nlm.nih.gov/articles/PMC4863427\",\n",
    "    \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7223160\",\n",
    "    \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7264388\",\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/125.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://pmc.ncbi.nlm.nih.gov/\",\n",
    "}\n",
    "\n",
    "for url in urls:\n",
    "    pmcid = url.split(\"/\")[-1]\n",
    "    outfile = Path(f\"{pmcid}.html\")\n",
    "    with requests.Session() as s:\n",
    "        s.headers.update(headers)\n",
    "        resp = s.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        if not resp.encoding:\n",
    "            resp.encoding = \"utf-8\"\n",
    "        outfile.write_text(resp.text, encoding=resp.encoding)\n",
    "    print(f\"Saved {outfile.resolve()} ({outfile.stat().st_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662a8432-f747-488e-999f-675b13decccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to b11504006_info_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Extraction handles multi-row headers (e.g., 'Coordinates' with x, y, z underneath), flattens headers, and extracts correct columns, including negative values\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "html_files = [\n",
    "    'PMC4863427.html',\n",
    "    'PMC7223160.html',\n",
    "    'PMC7264388.html',\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "def extract_keywords(soup, html_text):\n",
    "    meta_keywords = soup.find('meta', attrs={'name': 'citation_keywords'})\n",
    "    if meta_keywords and meta_keywords.get('content'):\n",
    "        return meta_keywords.get('content')\n",
    "    for tag in soup.find_all(['b', 'strong', 'h3', 'h4', 'p', 'span']):\n",
    "        if tag.get_text(strip=True).lower().startswith('keywords'):\n",
    "            next_text = ''\n",
    "            text = tag.get_text(separator=' ', strip=True)\n",
    "            if ':' in text:\n",
    "                next_text = text.split(':', 1)[1].strip()\n",
    "            if not next_text:\n",
    "                sibling = tag.find_next_sibling(text=True)\n",
    "                if sibling:\n",
    "                    next_text = sibling.strip()\n",
    "            if not next_text and tag.parent:\n",
    "                sibling = tag.parent.find_next_sibling(text=True)\n",
    "                if sibling:\n",
    "                    next_text = sibling.strip()\n",
    "            if next_text:\n",
    "                return next_text\n",
    "    kw_match = re.search(r'Keywords?[:\\s]+([\\w\\s;,-]+)', html_text, re.IGNORECASE)\n",
    "    if kw_match:\n",
    "        return kw_match.group(1).strip()\n",
    "    return ''\n",
    "\n",
    "def is_number(val):\n",
    "    try:\n",
    "        # Remove spaces between minus and number, e.g., '- 36' -> '-36'\n",
    "        val = re.sub(r'^-\\s+', '-', val.strip())\n",
    "        float(val)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def find_xyz_indices_from_multiline_header(header_rows):\n",
    "    n_cols = max(len(row) for row in header_rows)\n",
    "    flat_headers = [''] * n_cols\n",
    "    for row in header_rows:\n",
    "        for i in range(n_cols):\n",
    "            part = row[i].strip().lower() if i < len(row) else ''\n",
    "            if part:\n",
    "                if flat_headers[i]:\n",
    "                    flat_headers[i] += ' ' + part\n",
    "                else:\n",
    "                    flat_headers[i] = part\n",
    "    x_idx = y_idx = z_idx = None\n",
    "    for i, h in enumerate(flat_headers):\n",
    "        if x_idx is None and re.search(r'\\bx\\b', h):\n",
    "            x_idx = i\n",
    "        elif y_idx is None and re.search(r'\\by\\b', h):\n",
    "            y_idx = i\n",
    "        elif z_idx is None and re.search(r'\\bz\\b', h):\n",
    "            z_idx = i\n",
    "    if None not in (x_idx, y_idx, z_idx):\n",
    "        return x_idx, y_idx, z_idx\n",
    "    return None\n",
    "\n",
    "for html_file in html_files:\n",
    "    html_path = Path(html_file)\n",
    "    if not html_path.exists():\n",
    "        print(f\"File not found: {html_file}\")\n",
    "        continue\n",
    "    html_text = html_path.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "    pmcid = re.search(r'PMC\\d+', html_file).group() if re.search(r'PMC\\d+', html_file) else ''\n",
    "\n",
    "    pmid = ''\n",
    "    meta_pmid = soup.find('meta', attrs={'name': 'citation_pmid'})\n",
    "    if meta_pmid:\n",
    "        pmid = meta_pmid.get('content')\n",
    "    else:\n",
    "        pmid_match = re.search(r'PMID[:\\s]+(\\d+)', html_text)\n",
    "        if pmid_match:\n",
    "            pmid = pmid_match.group(1)\n",
    "\n",
    "    keywords = extract_keywords(soup, html_text)\n",
    "\n",
    "    tables = soup.find_all('table')\n",
    "    for table_idx, table in enumerate(tables, 1):\n",
    "        rows = table.find_all('tr')\n",
    "        if not rows:\n",
    "            continue\n",
    "        header_rows = []\n",
    "        for row in rows[:2]:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            if all(cell.name == 'th' for cell in cells):\n",
    "                header_rows.append([cell.get_text(strip=True) for cell in cells])\n",
    "        if not header_rows:\n",
    "            header_rows = [[cell.get_text(strip=True) for cell in rows[0].find_all(['th', 'td'])]]\n",
    "        xyz_indices = find_xyz_indices_from_multiline_header(header_rows)\n",
    "        if xyz_indices:\n",
    "            x_idx, y_idx, z_idx = xyz_indices\n",
    "            for row in rows[len(header_rows):]:\n",
    "                cols = [td.get_text(strip=True) for td in row.find_all(['td', 'th'])]\n",
    "                if len(cols) > max(x_idx, y_idx, z_idx):\n",
    "                    x_val, y_val, z_val = cols[x_idx], cols[y_idx], cols[z_idx]\n",
    "                    # Remove spaces between minus and number for each value\n",
    "                    x_val = re.sub(r'^-\\s+', '-', x_val.strip())\n",
    "                    y_val = re.sub(r'^-\\s+', '-', y_val.strip())\n",
    "                    z_val = re.sub(r'^-\\s+', '-', z_val.strip())\n",
    "                    if all(is_number(v) for v in [x_val, y_val, z_val]):\n",
    "                        x = float(x_val)\n",
    "                        y = float(y_val)\n",
    "                        z = float(z_val)\n",
    "                        results.append({\n",
    "                            'PMID': pmid,\n",
    "                            'PMCID': pmcid.replace('PMC',''),\n",
    "                            'Keywords': keywords,\n",
    "                            'Table': table_idx,\n",
    "                            'X': x,\n",
    "                            'Y': y,\n",
    "                            'Z': z,\n",
    "                        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('b11504006_info_data.csv', index=False)\n",
    "print('Saved to b11504006_info_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cad97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
